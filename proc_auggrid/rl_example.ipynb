{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://leedakyeong.tistory.com/entry/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%9C%BC%EB%A1%9C-%EA%B3%B5%EC%A0%95-%EC%9E%90%EB%8F%99-%EC%A0%9C%EC%96%B4-%EC%8B%9C%EB%AE%AC%EB%A0%88%EC%9D%B4%EC%85%98-%ED%95%B4%EB%B3%B4%EA%B8%B0\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms as T \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "temp = [random.randint(-10, 100) for _ in range(100)]\n",
    "property = [i*3.5 + 10 + random.random()*2 for i in temp]\n",
    "\n",
    "data = pd.DataFrame({'x':temp, 'y':property})\n",
    "data.plot(x='x', y='y', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.50132294]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression().fit(np.array(temp).reshape(-1, 1), np.array(property).reshape(-1, 1))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, z, setPoint):\n",
    "        self.z = z\n",
    "        self.setPoint = setPoint\n",
    "        self.terminated = False\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        yPred = model.predict(np.array([self.z]).reshape(-1,1)).item()\n",
    "        self.state = torch.tensor([self.setPoint - yPred])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.z -= 1\n",
    "        elif action == 1:\n",
    "            self.z += 1\n",
    "\n",
    "        if self.z < -10 or self.z > 100:\n",
    "            reward = -100. \n",
    "            self.terminated = True\n",
    "            return None, torch.tensor([reward]), self.terminated\n",
    "\n",
    "        self.state = torch.tensor([self.setPoint - model.predict(np.array([self.z]).reshape(-1, 1)).item()])\n",
    "        if abs(self.state) <= 5:\n",
    "            self.terminated = True\n",
    "        reward = 1/abs(self.state)*5 \n",
    "        return self.state, reward, self.terminated \n",
    "\n",
    "    def render(self):\n",
    "        return self.z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 16, bias=True)\n",
    "        self.linear2 = nn.Linear(16, outputs, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return torch.unsqueeze(F.log_softmax(x, dim=0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "n_actions = 2\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            print('===state',  policy_net(state))\n",
    "            print(policy_net(state).max(1).indices.view(1,1))\n",
    "            return policy_net(state).max(1).indices.view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states.reshape(non_final_next_states.size()[0], 1)).squeeze().max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setPoint 304 for adjust\n",
      "0 - current x at  14 Action :  Up y :  tensor([244.0372]) reward : 0.02\n",
      "1 - current x at  15 Action :  Up y :  tensor([240.5359]) reward : 0.02\n",
      "===state tensor([[  0.0000, -30.0598]])\n",
      "tensor([[0]])\n",
      "2 - current x at  14 Action :  Down y :  tensor([244.0372]) reward : 0.02\n",
      "3 - current x at  15 Action :  Up y :  tensor([240.5359]) reward : 0.02\n",
      "4 - current x at  16 Action :  Up y :  tensor([237.0345]) reward : 0.02\n",
      "===state tensor([[  0.0000, -29.6251]])\n",
      "tensor([[0]])\n",
      "5 - current x at  15 Action :  Down y :  tensor([240.5359]) reward : 0.02\n",
      "6 - current x at  14 Action :  Down y :  tensor([244.0372]) reward : 0.02\n",
      "7 - current x at  15 Action :  Up y :  tensor([240.5359]) reward : 0.02\n",
      "8 - current x at  14 Action :  Down y :  tensor([244.0372]) reward : 0.02\n",
      "9 - current x at  13 Action :  Down y :  tensor([247.5385]) reward : 0.02\n",
      "10 - current x at  14 Action :  Up y :  tensor([244.0372]) reward : 0.02\n",
      "11 - current x at  13 Action :  Down y :  tensor([247.5385]) reward : 0.02\n",
      "===state tensor([[  0.0000, -30.9291]])\n",
      "tensor([[0]])\n",
      "12 - current x at  12 Action :  Down y :  tensor([251.0398]) reward : 0.02\n",
      "13 - current x at  11 Action :  Down y :  tensor([254.5412]) reward : 0.02\n",
      "===state tensor([[  0.0000, -31.7984]])\n",
      "tensor([[0]])\n",
      "14 - current x at  10 Action :  Down y :  tensor([258.0425]) reward : 0.02\n",
      "===state tensor([[  0.0000, -32.2331]])\n",
      "tensor([[0]])\n",
      "15 - current x at  9 Action :  Down y :  tensor([261.5438]) reward : 0.02\n",
      "===state tensor([[  0.0000, -32.6677]])\n",
      "tensor([[0]])\n",
      "16 - current x at  8 Action :  Down y :  tensor([265.0451]) reward : 0.02\n",
      "===state tensor([[  0.0000, -33.1024]])\n",
      "tensor([[0]])\n",
      "17 - current x at  7 Action :  Down y :  tensor([268.5464]) reward : 0.02\n",
      "18 - current x at  8 Action :  Up y :  tensor([265.0451]) reward : 0.02\n",
      "19 - current x at  7 Action :  Down y :  tensor([268.5464]) reward : 0.02\n",
      "20 - current x at  6 Action :  Down y :  tensor([272.0478]) reward : 0.02\n",
      "21 - current x at  7 Action :  Up y :  tensor([268.5464]) reward : 0.02\n",
      "22 - current x at  6 Action :  Down y :  tensor([272.0478]) reward : 0.02\n",
      "23 - current x at  5 Action :  Down y :  tensor([275.5491]) reward : 0.02\n",
      "24 - current x at  4 Action :  Down y :  tensor([279.0504]) reward : 0.02\n",
      "===state tensor([[  0.0000, -34.8410]])\n",
      "tensor([[0]])\n",
      "25 - current x at  3 Action :  Down y :  tensor([282.5518]) reward : 0.02\n",
      "===state tensor([[  0.0000, -35.2757]])\n",
      "tensor([[0]])\n",
      "26 - current x at  2 Action :  Down y :  tensor([286.0531]) reward : 0.02\n",
      "===state tensor([[  0.0000, -35.7103]])\n",
      "tensor([[0]])\n",
      "27 - current x at  1 Action :  Down y :  tensor([289.5544]) reward : 0.02\n",
      "28 - current x at  2 Action :  Up y :  tensor([286.0531]) reward : 0.02\n",
      "===state tensor([[  0.0000, -35.7103]])\n",
      "tensor([[0]])\n",
      "29 - current x at  1 Action :  Down y :  tensor([289.5544]) reward : 0.02\n",
      "30 - current x at  0 Action :  Down y :  tensor([293.0557]) reward : 0.02\n",
      "31 - current x at  -1 Action :  Down y :  tensor([296.5570]) reward : 0.02\n",
      "32 - current x at  0 Action :  Up y :  tensor([293.0557]) reward : 0.02\n",
      "33 - current x at  -1 Action :  Down y :  tensor([296.5570]) reward : 0.02\n",
      "34 - current x at  0 Action :  Up y :  tensor([293.0557]) reward : 0.02\n",
      "35 - current x at  -1 Action :  Down y :  tensor([296.5570]) reward : 0.02\n",
      "36 - current x at  0 Action :  Up y :  tensor([293.0557]) reward : 0.02\n",
      "37 - current x at  -1 Action :  Down y :  tensor([296.5570]) reward : 0.02\n",
      "38 - current x at  0 Action :  Up y :  tensor([293.0557]) reward : 0.02\n",
      "39 - current x at  -1 Action :  Down y :  tensor([296.5570]) reward : 0.02\n",
      "40 - current x at  -2 Action :  Down y :  tensor([300.0583]) reward : 0.02\n",
      "===state tensor([[  0.0000, -37.4490]])\n",
      "tensor([[0]])\n",
      "41 - current x at  -3 Action :  Down y :  tensor([303.5597]) reward : 0.02\n",
      "42 - current x at  -2 Action :  Up y :  tensor([300.0583]) reward : 0.02\n",
      "43 - current x at  -1 Action :  Up y :  tensor([296.5570]) reward : 0.02\n",
      "44 - current x at  0 Action :  Up y :  tensor([293.0557]) reward : 0.02\n",
      "45 - current x at  -1 Action :  Down y :  tensor([296.5570]) reward : 0.02\n",
      "46 - current x at  0 Action :  Up y :  tensor([293.0557]) reward : 0.02\n",
      "47 - current x at  1 Action :  Up y :  tensor([289.5544]) reward : 0.02\n",
      "===state tensor([[  0.0000, -36.1450]])\n",
      "tensor([[0]])\n",
      "48 - current x at  0 Action :  Down y :  tensor([293.0557]) reward : 0.02\n",
      "49 - current x at  1 Action :  Up y :  tensor([289.5544]) reward : 0.02\n",
      "===state tensor([[  0.0000, -36.1450]])\n",
      "tensor([[0]])\n",
      "50 - current x at  0 Action :  Down y :  tensor([293.0557]) reward : 0.02\n",
      "51 - current x at  -1 Action :  Down y :  tensor([296.5570]) reward : 0.02\n",
      "52 - current x at  -2 Action :  Down y :  tensor([300.0583]) reward : 0.02\n",
      "===state tensor([[  0.0000, -37.4490]])\n",
      "tensor([[0]])\n",
      "53 - current x at  -3 Action :  Down y :  tensor([303.5597]) reward : 0.02\n",
      "54 - current x at  -2 Action :  Up y :  tensor([300.0583]) reward : 0.02\n",
      "55 - current x at  -3 Action :  Down y :  tensor([303.5597]) reward : 0.02\n",
      "56 - current x at  -4 Action :  Down y :  tensor([307.0610]) reward : 0.02\n",
      "57 - current x at  -5 Action :  Down y :  tensor([310.5623]) reward : 0.02\n",
      "58 - current x at  -6 Action :  Down y :  tensor([314.0637]) reward : 0.02\n",
      "59 - current x at  -5 Action :  Up y :  tensor([310.5623]) reward : 0.02\n",
      "60 - current x at  -4 Action :  Up y :  tensor([307.0610]) reward : 0.02\n",
      "===state tensor([[  0.0000, -38.3183]])\n",
      "tensor([[0]])\n",
      "61 - current x at  -5 Action :  Down y :  tensor([310.5623]) reward : 0.02\n",
      "===state tensor([[  0.0000, -38.7529]])\n",
      "tensor([[0]])\n",
      "62 - current x at  -6 Action :  Down y :  tensor([314.0637]) reward : 0.02\n",
      "63 - current x at  -7 Action :  Down y :  tensor([317.5650]) reward : 0.02\n",
      "===state tensor([[  0.0000, -39.6223]])\n",
      "tensor([[0]])\n",
      "64 - current x at  -8 Action :  Down y :  tensor([321.0663]) reward : 0.02\n",
      "65 - current x at  -7 Action :  Up y :  tensor([317.5650]) reward : 0.02\n",
      "66 - current x at  -6 Action :  Up y :  tensor([314.0637]) reward : 0.02\n",
      "67 - current x at  -5 Action :  Up y :  tensor([310.5623]) reward : 0.02\n",
      "68 - current x at  -6 Action :  Down y :  tensor([314.0637]) reward : 0.02\n",
      "69 - current x at  -7 Action :  Down y :  tensor([317.5650]) reward : 0.02\n",
      "70 - current x at  -8 Action :  Down y :  tensor([321.0663]) reward : 0.02\n",
      "===state tensor([[  0.0000, -40.0569]])\n",
      "tensor([[0]])\n",
      "71 - current x at  -9 Action :  Down y :  tensor([324.5676]) reward : 0.02\n",
      "===state tensor([[  0.0000, -40.4916]])\n",
      "tensor([[0]])\n",
      "72 - current x at  -10 Action :  Down y :  tensor([328.0689]) reward : 0.02\n",
      "73 - current x at  -11 Action :  Down y :  None reward : -100.0\n",
      "Fail!!\n",
      "Complete!!\n",
      "setPoint 23 for adjust\n",
      "0 - current x at  7 Action :  Down y :  tensor([-12.4535]) reward : 0.4\n",
      "===state tensor([[-0.0488, -3.0440]])\n",
      "tensor([[0]])\n",
      "1 - current x at  6 Action :  Down y :  tensor([-8.9522]) reward : 0.56\n",
      "2 - current x at  5 Action :  Down y :  tensor([-5.4509]) reward : 0.92\n",
      "3 - current x at  6 Action :  Up y :  tensor([-8.9522]) reward : 0.56\n",
      "===state tensor([[-0.1099, -2.2624]])\n",
      "tensor([[0]])\n",
      "4 - current x at  5 Action :  Down y :  tensor([-5.4509]) reward : 0.92\n",
      "5 - current x at  4 Action :  Down y :  tensor([-1.9496]) reward : 2.56\n",
      "Success!!\n",
      "Complete!!\n",
      "setPoint 200 for adjust\n",
      "0 - current x at  73 Action :  Down y :  tensor([-66.5409]) reward : 0.08\n",
      "===state tensor([[-1.1921e-07, -1.6015e+01]])\n",
      "tensor([[0]])\n",
      "1 - current x at  72 Action :  Down y :  tensor([-63.0395]) reward : 0.08\n",
      "2 - current x at  73 Action :  Up y :  tensor([-66.5409]) reward : 0.08\n",
      "3 - current x at  74 Action :  Up y :  tensor([-70.0422]) reward : 0.07\n",
      "===state tensor([[  0.0000, -16.8573]])\n",
      "tensor([[0]])\n",
      "4 - current x at  73 Action :  Down y :  tensor([-66.5409]) reward : 0.08\n",
      "5 - current x at  72 Action :  Down y :  tensor([-63.0395]) reward : 0.08\n",
      "===state tensor([[-2.3842e-07, -1.5172e+01]])\n",
      "tensor([[0]])\n",
      "6 - current x at  71 Action :  Down y :  tensor([-59.5382]) reward : 0.08\n",
      "7 - current x at  70 Action :  Down y :  tensor([-56.0369]) reward : 0.09\n",
      "===state tensor([[-1.4305e-06, -1.3486e+01]])\n",
      "tensor([[0]])\n",
      "8 - current x at  69 Action :  Down y :  tensor([-52.5356]) reward : 0.1\n",
      "===state tensor([[-3.2186e-06, -1.2643e+01]])\n",
      "tensor([[0]])\n",
      "9 - current x at  68 Action :  Down y :  tensor([-49.0342]) reward : 0.1\n",
      "10 - current x at  69 Action :  Up y :  tensor([-52.5356]) reward : 0.1\n",
      "11 - current x at  68 Action :  Down y :  tensor([-49.0342]) reward : 0.1\n",
      "===state tensor([[-7.5102e-06, -1.1801e+01]])\n",
      "tensor([[0]])\n",
      "12 - current x at  67 Action :  Down y :  tensor([-45.5329]) reward : 0.11\n",
      "===state tensor([[-1.7404e-05, -1.0958e+01]])\n",
      "tensor([[0]])\n",
      "13 - current x at  66 Action :  Down y :  tensor([-42.0316]) reward : 0.12\n",
      "===state tensor([[-4.0411e-05, -1.0115e+01]])\n",
      "tensor([[0]])\n",
      "14 - current x at  65 Action :  Down y :  tensor([-38.5303]) reward : 0.13\n",
      "===state tensor([[-9.4052e-05, -9.2722e+00]])\n",
      "tensor([[0]])\n",
      "15 - current x at  64 Action :  Down y :  tensor([-35.0290]) reward : 0.14\n",
      "===state tensor([[-2.1837e-04, -8.4295e+00]])\n",
      "tensor([[0]])\n",
      "16 - current x at  63 Action :  Down y :  tensor([-31.5276]) reward : 0.16\n",
      "17 - current x at  62 Action :  Down y :  tensor([-28.0263]) reward : 0.18\n",
      "===state tensor([[-1.1776e-03, -6.7449e+00]])\n",
      "tensor([[0]])\n",
      "18 - current x at  61 Action :  Down y :  tensor([-24.5250]) reward : 0.2\n",
      "19 - current x at  60 Action :  Down y :  tensor([-21.0237]) reward : 0.24\n",
      "20 - current x at  59 Action :  Down y :  tensor([-17.5223]) reward : 0.29\n",
      "===state tensor([[-0.0147, -4.2300]])\n",
      "tensor([[0]])\n",
      "21 - current x at  58 Action :  Down y :  tensor([-14.0210]) reward : 0.36\n",
      "22 - current x at  57 Action :  Down y :  tensor([-10.5197]) reward : 0.48\n",
      "23 - current x at  56 Action :  Down y :  tensor([-7.0184]) reward : 0.71\n",
      "24 - current x at  55 Action :  Down y :  tensor([-3.5170]) reward : 1.42\n",
      "Success!!\n",
      "Complete!!\n",
      "setPoint 50 for adjust\n",
      "===state tensor([[-0.4028, -1.1040]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n",
      "0 - current x at  9 Action :  Down y :  tensor([7.5438]) reward : 0.66\n",
      "1 - current x at  10 Action :  Up y :  tensor([4.0425]) reward : 1.24\n",
      "Success!!\n",
      "Complete!!\n",
      "setPoint 334 for adjust\n",
      "===state tensor([[  0.0000, -17.2670]])\n",
      "tensor([[0]])\n",
      "0 - current x at  52 Action :  Down y :  tensor([140.9869]) reward : 0.04\n",
      "1 - current x at  51 Action :  Down y :  tensor([144.4883]) reward : 0.03\n",
      "2 - current x at  52 Action :  Up y :  tensor([140.9869]) reward : 0.04\n",
      "3 - current x at  51 Action :  Down y :  tensor([144.4883]) reward : 0.03\n",
      "4 - current x at  52 Action :  Up y :  tensor([140.9869]) reward : 0.04\n",
      "===state tensor([[  0.0000, -17.7017]])\n",
      "tensor([[0]])\n",
      "5 - current x at  51 Action :  Down y :  tensor([144.4883]) reward : 0.03\n",
      "===state tensor([[  0.0000, -18.1363]])\n",
      "tensor([[0]])\n",
      "6 - current x at  50 Action :  Down y :  tensor([147.9896]) reward : 0.03\n",
      "7 - current x at  51 Action :  Up y :  tensor([144.4883]) reward : 0.03\n",
      "===state tensor([[  0.0000, -18.1363]])\n",
      "tensor([[0]])\n",
      "8 - current x at  50 Action :  Down y :  tensor([147.9896]) reward : 0.03\n",
      "===state tensor([[  0.0000, -18.5710]])\n",
      "tensor([[0]])\n",
      "9 - current x at  49 Action :  Down y :  tensor([151.4909]) reward : 0.03\n",
      "===state tensor([[  0.0000, -19.0056]])\n",
      "tensor([[0]])\n",
      "10 - current x at  48 Action :  Down y :  tensor([154.9922]) reward : 0.03\n",
      "11 - current x at  49 Action :  Up y :  tensor([151.4909]) reward : 0.03\n",
      "===state tensor([[  0.0000, -19.0056]])\n",
      "tensor([[0]])\n",
      "12 - current x at  48 Action :  Down y :  tensor([154.9922]) reward : 0.03\n",
      "===state tensor([[  0.0000, -19.4403]])\n",
      "tensor([[0]])\n",
      "13 - current x at  47 Action :  Down y :  tensor([158.4935]) reward : 0.03\n",
      "===state tensor([[  0.0000, -19.8750]])\n",
      "tensor([[0]])\n",
      "14 - current x at  46 Action :  Down y :  tensor([161.9949]) reward : 0.03\n",
      "===state tensor([[  0.0000, -20.3096]])\n",
      "tensor([[0]])\n",
      "15 - current x at  45 Action :  Down y :  tensor([165.4962]) reward : 0.03\n",
      "16 - current x at  46 Action :  Up y :  tensor([161.9949]) reward : 0.03\n",
      "===state tensor([[  0.0000, -20.3096]])\n",
      "tensor([[0]])\n",
      "17 - current x at  45 Action :  Down y :  tensor([165.4962]) reward : 0.03\n",
      "===state tensor([[  0.0000, -20.7443]])\n",
      "tensor([[0]])\n",
      "18 - current x at  44 Action :  Down y :  tensor([168.9975]) reward : 0.03\n",
      "19 - current x at  45 Action :  Up y :  tensor([165.4962]) reward : 0.03\n",
      "===state tensor([[  0.0000, -20.7443]])\n",
      "tensor([[0]])\n",
      "20 - current x at  44 Action :  Down y :  tensor([168.9975]) reward : 0.03\n",
      "21 - current x at  43 Action :  Down y :  tensor([172.4988]) reward : 0.03\n",
      "===state tensor([[  0.0000, -21.6136]])\n",
      "tensor([[0]])\n",
      "22 - current x at  42 Action :  Down y :  tensor([176.0002]) reward : 0.03\n",
      "23 - current x at  43 Action :  Up y :  tensor([172.4988]) reward : 0.03\n",
      "===state tensor([[  0.0000, -21.6136]])\n",
      "tensor([[0]])\n",
      "24 - current x at  42 Action :  Down y :  tensor([176.0002]) reward : 0.03\n",
      "25 - current x at  43 Action :  Up y :  tensor([172.4988]) reward : 0.03\n",
      "26 - current x at  42 Action :  Down y :  tensor([176.0002]) reward : 0.03\n",
      "===state tensor([[  0.0000, -22.0482]])\n",
      "tensor([[0]])\n",
      "27 - current x at  41 Action :  Down y :  tensor([179.5015]) reward : 0.03\n",
      "===state tensor([[  0.0000, -22.4829]])\n",
      "tensor([[0]])\n",
      "28 - current x at  40 Action :  Down y :  tensor([183.0028]) reward : 0.03\n",
      "===state tensor([[  0.0000, -22.9176]])\n",
      "tensor([[0]])\n",
      "29 - current x at  39 Action :  Down y :  tensor([186.5041]) reward : 0.03\n",
      "30 - current x at  40 Action :  Up y :  tensor([183.0028]) reward : 0.03\n",
      "31 - current x at  41 Action :  Up y :  tensor([179.5015]) reward : 0.03\n",
      "===state tensor([[  0.0000, -22.4829]])\n",
      "tensor([[0]])\n",
      "32 - current x at  40 Action :  Down y :  tensor([183.0028]) reward : 0.03\n",
      "33 - current x at  41 Action :  Up y :  tensor([179.5015]) reward : 0.03\n",
      "===state tensor([[  0.0000, -22.4829]])\n",
      "tensor([[0]])\n",
      "34 - current x at  40 Action :  Down y :  tensor([183.0028]) reward : 0.03\n",
      "===state tensor([[  0.0000, -22.9176]])\n",
      "tensor([[0]])\n",
      "35 - current x at  39 Action :  Down y :  tensor([186.5041]) reward : 0.03\n",
      "===state tensor([[  0.0000, -23.3522]])\n",
      "tensor([[0]])\n",
      "36 - current x at  38 Action :  Down y :  tensor([190.0054]) reward : 0.03\n",
      "37 - current x at  39 Action :  Up y :  tensor([186.5041]) reward : 0.03\n",
      "===state tensor([[  0.0000, -23.3522]])\n",
      "tensor([[0]])\n",
      "38 - current x at  38 Action :  Down y :  tensor([190.0054]) reward : 0.03\n",
      "===state tensor([[  0.0000, -23.7869]])\n",
      "tensor([[0]])\n",
      "39 - current x at  37 Action :  Down y :  tensor([193.5068]) reward : 0.03\n",
      "===state tensor([[  0.0000, -24.2215]])\n",
      "tensor([[0]])\n",
      "40 - current x at  36 Action :  Down y :  tensor([197.0081]) reward : 0.03\n",
      "===state tensor([[  0.0000, -24.6562]])\n",
      "tensor([[0]])\n",
      "41 - current x at  35 Action :  Down y :  tensor([200.5094]) reward : 0.02\n",
      "42 - current x at  36 Action :  Up y :  tensor([197.0081]) reward : 0.03\n",
      "43 - current x at  37 Action :  Up y :  tensor([193.5068]) reward : 0.03\n",
      "44 - current x at  38 Action :  Up y :  tensor([190.0054]) reward : 0.03\n",
      "===state tensor([[  0.0000, -23.7869]])\n",
      "tensor([[0]])\n",
      "45 - current x at  37 Action :  Down y :  tensor([193.5068]) reward : 0.03\n",
      "===state tensor([[  0.0000, -24.2215]])\n",
      "tensor([[0]])\n",
      "46 - current x at  36 Action :  Down y :  tensor([197.0081]) reward : 0.03\n",
      "===state tensor([[  0.0000, -24.6562]])\n",
      "tensor([[0]])\n",
      "47 - current x at  35 Action :  Down y :  tensor([200.5094]) reward : 0.02\n",
      "===state tensor([[  0.0000, -25.0909]])\n",
      "tensor([[0]])\n",
      "48 - current x at  34 Action :  Down y :  tensor([204.0107]) reward : 0.02\n",
      "49 - current x at  33 Action :  Down y :  tensor([207.5121]) reward : 0.02\n",
      "===state tensor([[  0.0000, -25.9602]])\n",
      "tensor([[0]])\n",
      "50 - current x at  32 Action :  Down y :  tensor([211.0134]) reward : 0.02\n",
      "51 - current x at  33 Action :  Up y :  tensor([207.5121]) reward : 0.02\n",
      "===state tensor([[  0.0000, -25.9602]])\n",
      "tensor([[0]])\n",
      "52 - current x at  32 Action :  Down y :  tensor([211.0134]) reward : 0.02\n",
      "===state tensor([[  0.0000, -26.3948]])\n",
      "tensor([[0]])\n",
      "53 - current x at  31 Action :  Down y :  tensor([214.5147]) reward : 0.02\n",
      "===state tensor([[  0.0000, -26.8295]])\n",
      "tensor([[0]])\n",
      "54 - current x at  30 Action :  Down y :  tensor([218.0160]) reward : 0.02\n",
      "===state tensor([[  0.0000, -27.2641]])\n",
      "tensor([[0]])\n",
      "55 - current x at  29 Action :  Down y :  tensor([221.5173]) reward : 0.02\n",
      "===state tensor([[  0.0000, -27.6988]])\n",
      "tensor([[0]])\n",
      "56 - current x at  28 Action :  Down y :  tensor([225.0187]) reward : 0.02\n",
      "===state tensor([[  0.0000, -28.1335]])\n",
      "tensor([[0]])\n",
      "57 - current x at  27 Action :  Down y :  tensor([228.5200]) reward : 0.02\n",
      "58 - current x at  28 Action :  Up y :  tensor([225.0187]) reward : 0.02\n",
      "===state tensor([[  0.0000, -28.1335]])\n",
      "tensor([[0]])\n",
      "59 - current x at  27 Action :  Down y :  tensor([228.5200]) reward : 0.02\n",
      "===state tensor([[  0.0000, -28.5681]])\n",
      "tensor([[0]])\n",
      "60 - current x at  26 Action :  Down y :  tensor([232.0213]) reward : 0.02\n",
      "===state tensor([[  0.0000, -29.0028]])\n",
      "tensor([[0]])\n",
      "61 - current x at  25 Action :  Down y :  tensor([235.5226]) reward : 0.02\n",
      "===state tensor([[  0.0000, -29.4374]])\n",
      "tensor([[0]])\n",
      "62 - current x at  24 Action :  Down y :  tensor([239.0240]) reward : 0.02\n",
      "===state tensor([[  0.0000, -29.8721]])\n",
      "tensor([[0]])\n",
      "63 - current x at  23 Action :  Down y :  tensor([242.5253]) reward : 0.02\n",
      "===state tensor([[  0.0000, -30.3068]])\n",
      "tensor([[0]])\n",
      "64 - current x at  22 Action :  Down y :  tensor([246.0266]) reward : 0.02\n",
      "===state tensor([[  0.0000, -30.7414]])\n",
      "tensor([[0]])\n",
      "65 - current x at  21 Action :  Down y :  tensor([249.5279]) reward : 0.02\n",
      "===state tensor([[  0.0000, -31.1761]])\n",
      "tensor([[0]])\n",
      "66 - current x at  20 Action :  Down y :  tensor([253.0293]) reward : 0.02\n",
      "===state tensor([[  0.0000, -31.6107]])\n",
      "tensor([[0]])\n",
      "67 - current x at  19 Action :  Down y :  tensor([256.5306]) reward : 0.02\n",
      "68 - current x at  20 Action :  Up y :  tensor([253.0293]) reward : 0.02\n",
      "69 - current x at  19 Action :  Down y :  tensor([256.5306]) reward : 0.02\n",
      "70 - current x at  20 Action :  Up y :  tensor([253.0293]) reward : 0.02\n",
      "===state tensor([[  0.0000, -31.6107]])\n",
      "tensor([[0]])\n",
      "71 - current x at  19 Action :  Down y :  tensor([256.5306]) reward : 0.02\n",
      "72 - current x at  18 Action :  Down y :  tensor([260.0319]) reward : 0.02\n",
      "===state tensor([[  0.0000, -32.4800]])\n",
      "tensor([[0]])\n",
      "73 - current x at  17 Action :  Down y :  tensor([263.5332]) reward : 0.02\n",
      "===state tensor([[  0.0000, -32.9147]])\n",
      "tensor([[0]])\n",
      "74 - current x at  16 Action :  Down y :  tensor([267.0345]) reward : 0.02\n",
      "===state tensor([[  0.0000, -33.3494]])\n",
      "tensor([[0]])\n",
      "75 - current x at  15 Action :  Down y :  tensor([270.5359]) reward : 0.02\n",
      "===state tensor([[  0.0000, -33.7840]])\n",
      "tensor([[0]])\n",
      "76 - current x at  14 Action :  Down y :  tensor([274.0372]) reward : 0.02\n",
      "===state tensor([[  0.0000, -34.2187]])\n",
      "tensor([[0]])\n",
      "77 - current x at  13 Action :  Down y :  tensor([277.5385]) reward : 0.02\n",
      "78 - current x at  14 Action :  Up y :  tensor([274.0372]) reward : 0.02\n",
      "79 - current x at  13 Action :  Down y :  tensor([277.5385]) reward : 0.02\n",
      "80 - current x at  14 Action :  Up y :  tensor([274.0372]) reward : 0.02\n",
      "===state tensor([[  0.0000, -34.2187]])\n",
      "tensor([[0]])\n",
      "81 - current x at  13 Action :  Down y :  tensor([277.5385]) reward : 0.02\n",
      "===state tensor([[  0.0000, -34.6533]])\n",
      "tensor([[0]])\n",
      "82 - current x at  12 Action :  Down y :  tensor([281.0398]) reward : 0.02\n",
      "83 - current x at  11 Action :  Down y :  tensor([284.5412]) reward : 0.02\n",
      "84 - current x at  10 Action :  Down y :  tensor([288.0425]) reward : 0.02\n",
      "85 - current x at  9 Action :  Down y :  tensor([291.5438]) reward : 0.02\n",
      "===state tensor([[  0.0000, -36.3920]])\n",
      "tensor([[0]])\n",
      "86 - current x at  8 Action :  Down y :  tensor([295.0451]) reward : 0.02\n",
      "===state tensor([[  0.0000, -36.8266]])\n",
      "tensor([[0]])\n",
      "87 - current x at  7 Action :  Down y :  tensor([298.5464]) reward : 0.02\n",
      "===state tensor([[  0.0000, -37.2613]])\n",
      "tensor([[0]])\n",
      "88 - current x at  6 Action :  Down y :  tensor([302.0478]) reward : 0.02\n",
      "===state tensor([[  0.0000, -37.6959]])\n",
      "tensor([[0]])\n",
      "89 - current x at  5 Action :  Down y :  tensor([305.5491]) reward : 0.02\n",
      "90 - current x at  6 Action :  Up y :  tensor([302.0478]) reward : 0.02\n",
      "===state tensor([[  0.0000, -37.6959]])\n",
      "tensor([[0]])\n",
      "91 - current x at  5 Action :  Down y :  tensor([305.5491]) reward : 0.02\n",
      "92 - current x at  4 Action :  Down y :  tensor([309.0504]) reward : 0.02\n",
      "93 - current x at  5 Action :  Up y :  tensor([305.5491]) reward : 0.02\n",
      "94 - current x at  6 Action :  Up y :  tensor([302.0478]) reward : 0.02\n",
      "===state tensor([[  0.0000, -37.6959]])\n",
      "tensor([[0]])\n",
      "95 - current x at  5 Action :  Down y :  tensor([305.5491]) reward : 0.02\n",
      "===state tensor([[  0.0000, -38.1306]])\n",
      "tensor([[0]])\n",
      "96 - current x at  4 Action :  Down y :  tensor([309.0504]) reward : 0.02\n",
      "===state tensor([[  0.0000, -38.5653]])\n",
      "tensor([[0]])\n",
      "97 - current x at  3 Action :  Down y :  tensor([312.5518]) reward : 0.02\n",
      "98 - current x at  2 Action :  Down y :  tensor([316.0531]) reward : 0.02\n",
      "===state tensor([[  0.0000, -39.4346]])\n",
      "tensor([[0]])\n",
      "99 - current x at  1 Action :  Down y :  tensor([319.5544]) reward : 0.02\n",
      "===state tensor([[  0.0000, -39.8692]])\n",
      "tensor([[0]])\n",
      "100 - current x at  0 Action :  Down y :  tensor([323.0557]) reward : 0.02\n",
      "===state tensor([[  0.0000, -40.3039]])\n",
      "tensor([[0]])\n",
      "101 - current x at  -1 Action :  Down y :  tensor([326.5570]) reward : 0.02\n",
      "===state tensor([[  0.0000, -40.7385]])\n",
      "tensor([[0]])\n",
      "102 - current x at  -2 Action :  Down y :  tensor([330.0583]) reward : 0.02\n",
      "===state tensor([[  0.0000, -41.1732]])\n",
      "tensor([[0]])\n",
      "103 - current x at  -3 Action :  Down y :  tensor([333.5597]) reward : 0.01\n",
      "===state tensor([[  0.0000, -41.6079]])\n",
      "tensor([[0]])\n",
      "104 - current x at  -4 Action :  Down y :  tensor([337.0610]) reward : 0.01\n",
      "===state tensor([[  0.0000, -42.0425]])\n",
      "tensor([[0]])\n",
      "105 - current x at  -5 Action :  Down y :  tensor([340.5623]) reward : 0.01\n",
      "===state tensor([[  0.0000, -42.4772]])\n",
      "tensor([[0]])\n",
      "106 - current x at  -6 Action :  Down y :  tensor([344.0637]) reward : 0.01\n",
      "===state tensor([[  0.0000, -42.9118]])\n",
      "tensor([[0]])\n",
      "107 - current x at  -7 Action :  Down y :  tensor([347.5650]) reward : 0.01\n",
      "108 - current x at  -6 Action :  Up y :  tensor([344.0637]) reward : 0.01\n",
      "===state tensor([[  0.0000, -42.9118]])\n",
      "tensor([[0]])\n",
      "109 - current x at  -7 Action :  Down y :  tensor([347.5650]) reward : 0.01\n",
      "110 - current x at  -6 Action :  Up y :  tensor([344.0637]) reward : 0.01\n",
      "===state tensor([[  0.0000, -42.9118]])\n",
      "tensor([[0]])\n",
      "111 - current x at  -7 Action :  Down y :  tensor([347.5650]) reward : 0.01\n",
      "112 - current x at  -8 Action :  Down y :  tensor([351.0663]) reward : 0.01\n",
      "===state tensor([[  0.0000, -43.7812]])\n",
      "tensor([[0]])\n",
      "113 - current x at  -9 Action :  Down y :  tensor([354.5676]) reward : 0.01\n",
      "===state tensor([[  0.0000, -44.2158]])\n",
      "tensor([[0]])\n",
      "114 - current x at  -10 Action :  Down y :  tensor([358.0689]) reward : 0.01\n",
      "===state tensor([[  0.0000, -44.6505]])\n",
      "tensor([[0]])\n",
      "115 - current x at  -11 Action :  Down y :  None reward : -100.0\n",
      "Fail!!\n",
      "Complete!!\n",
      "setPoint 265 for adjust\n",
      "0 - current x at  1 Action :  Down y :  tensor([250.5544]) reward : 0.02\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m epMemory\u001b[38;5;241m.\u001b[39mappend([state, action, next_state, reward])\n\u001b[1;32m     24\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 25\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m:\n",
      "Cell \u001b[0;32mIn[38], line 15\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39maction)\n\u001b[1;32m     13\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mreward)\n\u001b[0;32m---> 15\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m next_state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(BATCH_SIZE, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     18\u001b[0m next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m target_net(non_final_next_states\u001b[38;5;241m.\u001b[39mreshape(non_final_next_states\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "def Action(x):\n",
    "    if x.item() == 0:\n",
    "        return \"Down\"\n",
    "    else:\n",
    "        return \"Up\"\n",
    "\n",
    "num_episodes = 100\n",
    "for i_episode in range(num_episodes):\n",
    "    epMemory = list()\n",
    "\n",
    "    z = random.randrange(-10, 100)\n",
    "    setPoint = random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])))\n",
    "    env = Environment(z=z, setPoint=setPoint)\n",
    "    state = env.reset()\n",
    "    print(\"setPoint\", setPoint, \"for adjust\")\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(torch.tensor([state]).float())\n",
    "        next_state, reward, done=env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        print(t, \"- current x at \", env.render(), 'Action : ', Action(action), 'y : ', next_state, 'reward :', round(reward.item(), 2))\n",
    "\n",
    "        epMemory.append([state, action, next_state, reward])\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "        \n",
    "        if done:\n",
    "            if env.render() >= -10 and env.render() <= 100:\n",
    "                _ = [memory.push(epMemory[i][0], epMemory[i][1], epMemory[i][2], epMemory[i][3]) for i in range(len(epMemory))]\n",
    "                print('Success!!')\n",
    "            else:\n",
    "                _ = [memory.push(epMemory[i][0], epMemory[i][1], epMemory[i][2], epMemory[i][3]) for i in range(len(epMemory))]\n",
    "                print('Fail!!')\n",
    "            break\n",
    "\n",
    "        if t >= 10000:\n",
    "            print('Terminated!!')\n",
    "            break\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print('Complete!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setPoint 173 to adjust\n",
      "===state tensor([[-4.1023, -0.0167]])\n",
      "tensor([[1]])\n",
      "Current x :  0 at  73 Action : Up setPoint - y :  tensor([-93.4701]) reward : 0.05\n",
      "===state tensor([[-4.2758, -0.0140]])\n",
      "tensor([[1]])\n",
      "Current x :  1 at  74 Action : Up setPoint - y :  tensor([-96.9679]) reward : 0.05\n",
      "===state tensor([[-4.4497, -0.0118]])\n",
      "tensor([[1]])\n",
      "Current x :  2 at  75 Action : Up setPoint - y :  tensor([-100.4658]) reward : 0.05\n",
      "===state tensor([[-4.6240, -0.0099]])\n",
      "tensor([[1]])\n",
      "Current x :  3 at  76 Action : Up setPoint - y :  tensor([-103.9636]) reward : 0.05\n",
      "===state tensor([[-4.7986, -0.0083]])\n",
      "tensor([[1]])\n",
      "Current x :  4 at  77 Action : Up setPoint - y :  tensor([-107.4614]) reward : 0.05\n",
      "===state tensor([[-4.9734, -0.0069]])\n",
      "tensor([[1]])\n",
      "Current x :  5 at  78 Action : Up setPoint - y :  tensor([-110.9593]) reward : 0.05\n",
      "===state tensor([[-5.1485, -0.0058]])\n",
      "tensor([[1]])\n",
      "Current x :  6 at  79 Action : Up setPoint - y :  tensor([-114.4571]) reward : 0.04\n",
      "===state tensor([[-5.3237e+00, -4.8865e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  7 at  80 Action : Up setPoint - y :  tensor([-117.9550]) reward : 0.04\n",
      "===state tensor([[-5.4991e+00, -4.0988e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  8 at  81 Action : Up setPoint - y :  tensor([-121.4528]) reward : 0.04\n",
      "===state tensor([[-5.6746e+00, -3.4378e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  9 at  82 Action : Up setPoint - y :  tensor([-124.9507]) reward : 0.04\n",
      "===state tensor([[-5.8503e+00, -2.8833e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  10 at  83 Action : Up setPoint - y :  tensor([-128.4485]) reward : 0.04\n",
      "===state tensor([[-6.0260e+00, -2.4181e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  11 at  84 Action : Up setPoint - y :  tensor([-131.9464]) reward : 0.04\n",
      "===state tensor([[-6.2017e+00, -2.0280e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  12 at  85 Action : Up setPoint - y :  tensor([-135.4442]) reward : 0.04\n",
      "===state tensor([[-6.3776e+00, -1.7006e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  13 at  86 Action : Up setPoint - y :  tensor([-138.9420]) reward : 0.04\n",
      "===state tensor([[-6.5535e+00, -1.4262e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  14 at  87 Action : Up setPoint - y :  tensor([-142.4399]) reward : 0.04\n",
      "===state tensor([[-6.7294e+00, -1.1959e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  15 at  88 Action : Up setPoint - y :  tensor([-145.9377]) reward : 0.03\n",
      "===state tensor([[-6.9054e+00, -1.0029e-03]])\n",
      "tensor([[1]])\n",
      "Current x :  16 at  89 Action : Up setPoint - y :  tensor([-149.4356]) reward : 0.03\n",
      "===state tensor([[-7.0814e+00, -8.4091e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  17 at  90 Action : Up setPoint - y :  tensor([-152.9334]) reward : 0.03\n",
      "===state tensor([[-7.2575e+00, -7.0511e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  18 at  91 Action : Up setPoint - y :  tensor([-156.4313]) reward : 0.03\n",
      "===state tensor([[-7.4335e+00, -5.9122e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  19 at  92 Action : Up setPoint - y :  tensor([-159.9291]) reward : 0.03\n",
      "===state tensor([[-7.6096e+00, -4.9579e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  20 at  93 Action : Up setPoint - y :  tensor([-163.4269]) reward : 0.03\n",
      "===state tensor([[-7.7857e+00, -4.1572e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  21 at  94 Action : Up setPoint - y :  tensor([-166.9248]) reward : 0.03\n",
      "===state tensor([[-7.9618e+00, -3.4863e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  22 at  95 Action : Up setPoint - y :  tensor([-170.4226]) reward : 0.03\n",
      "===state tensor([[-8.1379e+00, -2.9226e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  23 at  96 Action : Up setPoint - y :  tensor([-173.9205]) reward : 0.03\n",
      "===state tensor([[-8.3141e+00, -2.4506e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  24 at  97 Action : Up setPoint - y :  tensor([-177.4183]) reward : 0.03\n",
      "===state tensor([[-8.4902e+00, -2.0550e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  25 at  98 Action : Up setPoint - y :  tensor([-180.9162]) reward : 0.03\n",
      "===state tensor([[-8.6663e+00, -1.7236e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  26 at  99 Action : Up setPoint - y :  tensor([-184.4140]) reward : 0.03\n",
      "===state tensor([[-8.8425e+00, -1.4447e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  27 at  100 Action : Up setPoint - y :  tensor([-187.9118]) reward : 0.03\n",
      "===state tensor([[-9.0186e+00, -1.2111e-04]])\n",
      "tensor([[1]])\n",
      "Current x :  28 at  101 Action : Up setPoint - y :  None reward : -100.0\n",
      "Fail!!!\n"
     ]
    }
   ],
   "source": [
    "z = random.randrange(-10, 100)\n",
    "setPoint = random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])))\n",
    "\n",
    "env = Environment(z=z, setPoint=setPoint)\n",
    "state = torch.tensor([setPoint - model.predict(np.array([z]).reshape(-1, 1)).item()])\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('setPoint', setPoint, 'to adjust')\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(torch.tensor([state]).float())\n",
    "        next_state, reward, done=env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        print('Current x : ', t, 'at ', env.render(), 'Action :', Action(action), 'setPoint - y : ', next_state, 'reward :', round(reward.item(), 2))\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if env.render() >= -10 and env.render() <= 100:\n",
    "                print('Success!!')\n",
    "            else:\n",
    "                print('Fail!!!')\n",
    "\n",
    "            break    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
